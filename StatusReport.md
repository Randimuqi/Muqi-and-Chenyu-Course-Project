Interim Status Report

Project Overview

Our project investigates how monthly crime rates in Chicago relate to changes in the local housing market. Specifically, we integrate the City of Chicago DM1 crime dataset with the FRED CHXRHTNSA Case–Shiller HPI series for Chicago to explore whether changes in crime levels are associated with changes in the housing price index over time.

The overall workflow follows the course data lifecycle: we acquire at least two datasets from trustworthy sources, store and organize them in a structured repository, clean and integrate them into a single monthly time series, assess data quality, and eventually perform exploratory analysis and visualization. This status report summarizes our progress so far, especially on the acquisition, cleaning, and integration tasks scheduled for Weeks 7–11.

Task Update

For weeks 9-10, Our task was to download two datasets, organize them into a repository, and begin cleaning and preparation. We downloaded the DM1 crime dataset, named "DM1_Crime_data_set_chicago_20251007.csv", and the CHXRHTNSA HPI series dataset, named "CHXRHTNSA.csv". We created a clear directory structure on GitHub to distinguish between raw and processed data. "data/raw/CHXRHTNSA.csv" contains the raw HPI data, stored on GitHub. "data/raw/DM1_Crime_data_set_chicago_20251007.csv" contains the raw crime data, due to its large size (over 25MB), it was stored locally. "data/processed/" is used to store the processed tables. We created the script "scripts/prepare_crime_data.py". We read the DM1 CSV file from the "data/raw/" directory. We normalize the column names (removing spaces), examine and parse the "Date of Occurrence" column, create a `year_month` column using a monthly cycle, summarize the event counts by the `year_month` column, and finally save the result as "data/processed/crime_monthly.csv". This script can be run from the command line so that anyone with access to the raw data can regenerate the monthly crime statistics. We created the "scripts/prepare_hpi_data.py" script, which reads the "CHXRHTNSA.csv" file from "data/raw/". We examine "observation_date" as the date column and "CHXRHTNSA" as the value column, parse the date and construct the "year_month" column, and rename the HPI values ​​to "hpi_value". Finally, we save the processed HPI sequence to "data/processed/hpi_monthly.csv". Like the crime script, HPI allows for reproducible regeneration of the processed data. Overall, we successfully generated reusable scripts and monthly datasets for crime and HPI in weeks 9 and 10, which are stored in “data/processed/”.

Week 11's task was monthly integration and consistency checks. We created the script "scripts/integrate_crime_hpi.py" to process the already processed dataset. We read "data/processed/crime_monthly.csv" and "data/processed/hpi_monthly.csv" and then parsed the shared "year_month" column from both tables into dates. Next, we performed an inner join on the "year_month" column, keeping only months present in both datasets, and sorted the merged table by the "year_month" column. Finally, we saved the integrated dataset as "data/processed/crime_hpi_monthly.csv," containing three key variables: "year_month," "crime_count," and "hpi_value." We also performed basic consistency checks, verifying that the date ranges in the two datasets overlapped as expected, and checked the beginning of the integrated dataset to confirm that the counts and HPI values ​​were aligned by month. Upon completion of Week 11, we had a clean, integrated monthly time series, which would serve as input for all subsequent analyses and visualizations.

Updated Timeline and Description of changes

Our original project timeline was that in week 7, we would finalize the research topic and dataset. In week 8, we would complete and submit the project plan. Weeks 9 and 10 would be used for downloading, cleaning, and preparing the data. Week 11 will focus on integrating the datasets monthly and checking their consistency. Weeks 12 and 13 will be dedicated to analysis, correlation testing, and visualization. Finally, in weeks 14 and 15, we will revise the report and submit the final results, along with a classroom presentation. We are currently in week 12. Weeks 7 through 11 have been completed as planned. Due to heavy exam schedules for our team members in week 12, we have decided to postpone the tasks from weeks 12 and 13 to week 13 (during fall break) to complete them together, when we will have more free time. Weeks 14 and 15 will still proceed as planned.

Reproducibility details feedback on Milestone 2

Feedback on Milestone 2 indicated shortcomings in the reproducibility of our plan. Therefore, we implemented specific measures in this phase milestone. Instead of simply preparing data in temporary notebooks, we implemented the main cleaning and integration steps as reusable Python scripts ("prepare_crime_data.py", "prepare_hpi_data.py", and "integrate_crime_hpi.py"), stored in the "scripts/:" directory. These scripts can be run from the command line to regenerate the processed dataset in "data/processed/" based on the raw input. We explicitly distinguished between raw and processed data, thus clarifying which files are immutable inputs and which can be regenerated from the workflow. Regarding metadata, for future milestones, we plan to create a small data dictionary describing the key variables in the integrated dataset ("year_month", "crime_count", "hpi_value") and noting where the downloaded raw data should be placed. This document will be included in the final files and related metadata files.

Short summary of contributions

Muqi Su

For the current milestone, I set up the local development environment, including a Python virtual environment on my local machine and the project directory structure. I completed "prepare_crime_data.py" to clean and aggregate the DM1 crime dataset, generating monthly statistics. My teammate and I collaborated on "integrate_crime_hpi.py" to merge the monthly crime data table and the HPI data table into an integrated dataset. I was responsible for writing most of the Status Report, including a project overview, detailed task updates, timeline status updates and change descriptions, and responses to the reproducibility feedback from Milestone 2.

Chenyu Wang

For the current milestone, I completed "prepare_hpi_data.py" to parse the FRED CHXRHTNSA sequence and convert it into a monthly HPI dataset. My teammate and I completed "integrate_crime_hpi.py" to merge the monthly crime data table and the HPI data table into a single integrated dataset. I validated that the processed dataset covered the expected date range, making it suitable as input for subsequent analyses. I was responsible for writing the response to the reproducibility feedback for Milestone 2 in the Status Report.
